{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor: basic unit for data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor is the basic unit of data in tensorflow. It can be considered as an array with multiple dimensions:\n",
    "- scalar (array with 0 dimension)\n",
    "- vector (array with 1 dimension)\n",
    "- matrix (array with 2 dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.38893163, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "random_float = tf.random.uniform(shape=())             # Declare a random float (scalar).\n",
    "print(random_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0.], shape=(2,), dtype=float32)\n",
      "tf.Tensor([0 0], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "zero_vector = tf.zeros(shape=(2))                      # Declare a zero vector with two elements, default type: float.\n",
    "print(zero_vector)\n",
    "zero_vector1 = tf.zeros(shape=(2), dtype = tf.int32)   # # Declare two 2*2 constant matrices A and B, change float to int\n",
    "print(zero_vector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[5. 6.]\n",
      " [7. 8.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "A = tf.constant([[1., 2.], [3., 4.]])                  # Declare a 2×2 constant matrix\n",
    "print(A)\n",
    "B = tf.constant([[5., 6.], [7., 8.]])\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor have 3 important attributes: shape, data type and value. You can use the shape, data type attribute and the numpy() method to fetch them. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "<dtype: 'float32'>\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# View the shape, type and value of matrix A.\n",
    "print(A.shape)      \n",
    "print(A.dtype)      \n",
    "print(A.numpy())     #  numpy() method of a tensor is to return a NumPy array whose value equals the value of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 6.  8.]\n",
      " [10. 12.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[19. 22.]\n",
      " [43. 50.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "C = tf.add(A, B)            # Compute the elementwise sum of A and B.\n",
    "D = tf.matmul(A, B)         # Compute the multiplication of A and B.\n",
    "print(C)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation mechanism\n",
    "In machine learning, we often need to compute derivatives of functions. TensorFlow provides the powerful Automatic differentiation mechanism for computing derivatives. The following codes show how to use tf.GradientTape() to computer the derivative of the function $y(x) = x^2$ at $x = 3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=28, shape=(), dtype=float32, numpy=9.0>, <tf.Tensor: id=32, shape=(), dtype=float32, numpy=6.0>]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# variable can be used to differentiate by the automatic differentiation mechanism of TensorFlow by default, \n",
    "# which is often used to define parameters of ML models.\n",
    "x = tf.Variable(initial_value = 3.) \n",
    "\n",
    "# All calculation steps will be recorded within the context of tf.GradientTape() for differentiation.\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.square(x)\n",
    "    y = tf.square(x)\n",
    "    \n",
    "# Compute the derivative of y with respect to x.\n",
    "y_grad = tape.gradient(y, x)        \n",
    "print([y, y_grad])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more common case in machine learning is partial differentiation of multivariable functions as well as differentiation of vectors and matrices.\n",
    "\n",
    "The following codes show how to obtain the partial derivative of the function \n",
    "$$L(w, b) = \\|Xw + b - y\\|^2 $$ for w, b respectively by tf.GradientTape() where $$X = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}, y = \\begin{bmatrix} 1 \\\\ 2\\end{bmatrix}$$\n",
    "\n",
    "$$w = (1, 2)^T, b = 1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62.5, array([[35.],\n",
      "       [50.]], dtype=float32), 15.0]\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant([[1., 2.], [3., 4.]])\n",
    "y = tf.constant([[1.], [2.]])\n",
    "w = tf.Variable(initial_value = [[1.], [2.]])\n",
    "b = tf.Variable(initial_value = 1.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    L = 0.5 * tf.reduce_sum(tf.square(tf.matmul(X, w) + b - y))\n",
    "    \n",
    "w_grad, b_grad = tape.gradient(L, [w, b])              # Compute the partial derivative of L(w, b) with respect to w and b.\n",
    "print([L.numpy(), w_grad.numpy(), b_grad.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output we can see TensorFlow has helped us obtained that\n",
    "\n",
    "$$L((1, 2)^T, 1) = 62.5$$\n",
    "\n",
    "$$\\frac{\\partial L(w, b)}{\\partial w}|_{w = (1, 2)^T, b = 1} = \\begin{bmatrix} 35 \\\\ 50\\end{bmatrix}$$\n",
    "\n",
    "$$\\frac{\\partial L(w, b)}{\\partial b} |_{w = (1, 2)^T, b = 1} = 15$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.square() here squared each element of the input tensor without altering its shape. \n",
    "# tf.reduce_sum() summed up all the elements of the input tensor, outputing a scalar tensor with a none shape \n",
    "# (the dimensions for sum can be specified by the parameter axis, without which all elements will be summed up by default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A basic example: Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a practical problem. The estate price of a city between 2013 and 2017 are listed below:\n",
    "\n",
    "| Year | 2013 | 2014 | 2015 | 2016 | 2017 |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Price | 12000 | 14000 | 15000 | 16500 | 17500 |\n",
    "\n",
    "Now we wish to perform a linear regression on this data, that is, use the linar model $y = ax + b$ to fit the data above, where a and b are parameters yet to be determined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the data and conduct basic normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.25 0.5  0.75 1.  ]\n",
      "[0.         0.36363637 0.54545456 0.8181818  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_raw = np.array([2013, 2014, 2015, 2016, 2017], dtype = np.float32)\n",
    "y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype = np.float32)\n",
    "\n",
    "X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())                     # normalization X\n",
    "print(X)\n",
    "\n",
    "y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent to find the parameters a and b.\n",
    "\n",
    "To find a <font color=red>local minimum</font> of a multivariable function $f(x)$, the process of gradient descent is as follows:\n",
    "\n",
    "- Initialize the independent variable to $x_0$, $k=0$.\n",
    "- Iterate the following steps until the convergence criterion is met:\n",
    "    - Find the gradient $\\nabla f(x_k)$ of the function $f(x)$ with respect to the independent variable.\n",
    "\n",
    "    - Update the independent variable: $x_{k+1} = x_{k} - \\gamma \\nabla f(x_k)$  where $\\gamma$ is the learning rate (i.e. the “stride” in one gradient descent).\n",
    "\n",
    "    - $k \\leftarrow k+1$.\n",
    "\n",
    "Next, we consider how to programme to implement the gradient descent method to find the solution of the linear regression \n",
    "\n",
    "$$\\min_{a, b} L(a, b) = \\sum_{i=1}^n(ax_i + b - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression under numPy\n",
    "- np.dot(A, B) <--> A.dot(B): the dot product: actually it's matrix multiplication.\n",
    "- A * B：the multiplication of the elements in the responding position \n",
    "- np.sum() gets the sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9763702027872221 0.057564988311377796\n"
     ]
    }
   ],
   "source": [
    "# (1) initialize the parameters a and b\n",
    "a, b = 0, 0\n",
    "\n",
    "# (2) initialize the training epoches and learning rate\n",
    "num_epoch = 10000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# (3) training\n",
    "for e in range(num_epoch):\n",
    "    # (3.1) compute predicted value\n",
    "    y_pred = a * X + b\n",
    "    \n",
    "    # (3.1) Compute the gradient of loss function with respect to independent variables (model parameters) manually.\n",
    "    grad_a, grad_b = (y_pred - y).dot(X), (y_pred - y).sum()\n",
    "\n",
    "    # (3.3) Update parameters.\n",
    "    a, b = a - learning_rate * grad_a, b - learning_rate * grad_b\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you may have already noticed that there are two pain points for implementing ML models when using conventional scientific computing libraries:\n",
    "\n",
    "- You have to <font color=red>find the partial derivatives with respect to parameters by yourself often</font>. It may be easy for simple functions, but the process would be very painful or even impossible once the functions become complex.\n",
    "\n",
    "- You have to <font color=red>update the parameters according to the result of the derivative by yourself frequently</font>. Here we used gradient descent, the most fundamental approach, thus it was not hard updating parameters. However, the process would have been very complicated if you use more advanced approaches updating parameters (e.g., Adam or Adagrad).\n",
    "\n",
    "The emergence of DL frameworks such as TensorFlow has largely solved these problems and has brought considerable convenience for implementing ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression under TensorFlow\n",
    "TensorFlow Eager Execution Mode is quite similar with how NumPy worked above, while it provides a series of features which are rather crucial for deep learning, such as faster computation (GPU support), automatic differentiation, optimizers, etc. Here TensorFlow helps us accomplished two crucial tasks:\n",
    "\n",
    "- Using tape.gradient(ys, xs) to compute the gradient automatically\n",
    "- Using optimizer.apply_gradients(grads_and_vars) to update model parameters automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.97637> <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.057565063>\n"
     ]
    }
   ],
   "source": [
    "# (0) data\n",
    "X = tf.constant(X)\n",
    "y = tf.constant(y)\n",
    "\n",
    "# (1) initialize the parameters a and b\n",
    "a = tf.Variable(initial_value = 0.)\n",
    "b = tf.Variable(initial_value = 0.)\n",
    "variables = [a, b]\n",
    "\n",
    "# (2) initialize the training epoches, learning rate and optimizer\n",
    "num_epoch = 10000\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate = 1e-3)\n",
    "\n",
    "# (3) training\n",
    "for e in range(num_epoch):\n",
    "    # (3.1) Use tf.GradientTape() to record information about the gradient of the loss function.\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = a * X + b\n",
    "        loss = 0.5 * tf.reduce_sum(tf.square(y_pred - y))\n",
    "        \n",
    "    # (3.2) TensorFlow computes the gradients of the loss function with respect to independent variables (model parameters) automatically.\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    \n",
    "    # (3.3) TensorFlow updates parameters according to the gradient automatically.\n",
    "    optimizer.apply_gradients(grads_and_vars = zip(grads, variables))\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.keras.optimizers.SGD(learning_rate=1e-3): a gradient descent optimizer updates model parameters based on the calculated derivative result, thereby minimizing a certain loss function.\n",
    "- optimizer.apply_gradients: call the function apply_gradients() for doing so.\n",
    "- grads_and_vars: the variables to be updated (like variables in the codes above) and the partial derivatives of the loss function with respect to them (like grads in the codes above),\n",
    "- Specifically, you need to pass in a Python list here whose elements are (the partial derivative for the variable, the variable) pairs, e.g., [(grad_a, a), (grad_b, b)] in this case. \n",
    "- grads = tape.gradient(loss, variables): the partial derivatives of loss with respect to each variable in variables = [a, b] recorded in tape, which are grads = [grad_a, grad_b]. Then we used the zip() function in Python to assemble grads = [grad_a, grad_b] and variables = [a, b] together to get the parameters we needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "272.056px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
